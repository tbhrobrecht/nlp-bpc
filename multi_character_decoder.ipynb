{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c950be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "from pypinyin import pinyin, Style\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "#  DATA LOADING\n",
    "# =========================\n",
    "\n",
    "dataframe = pd.read_csv(\n",
    "    \"C:\\\\Users\\\\tbhro\\\\PycharmProjects\\\\nlp-bpc\\\\zho_data\\\\first_part.csv\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "zho_text = dataframe[\"text\"].tolist()\n",
    "zho_text_cleaned = [text.replace(\"\\r\\n\", \"\") for text in zho_text]\n",
    "\n",
    "# All pinyin-initial tokens you got from your model\n",
    "TOKEN_PATTERNS =  ['ds', 'zd', 'zy', 'js', 'zs', 'dy', 'zm', 'zg', 'zj', 'ys', 'dd', 'wm', 'bs', 'yg', 'zz', 'ss', 'yy', 'jd', 'ws', 'zl', 'yd', 'hs', 'ns', 'nm', 'ls', 'xx', 'hd', 'ld', 'yb', 'zb', 'xs', 'yj', 'gs', 'nd', 'hl', 'xd', 'wd', 'zh', 'ng', 'td', 'yw', 'zx', 'bd', 'zc', 'tm', 'll', 'ts', 'yx', 'nb', 'zw', 'qs', 'jj', 'my', 'xl', 'zn', 'cl', 'ms', 'cs', 'yh', 'yl', 'jl', 'nn', 'yq', 'jg', 'cd', 'md', 'wl', 'hb', 'jb', 'wb', 'ks', 'qd', 'gd', 'ql', 'hh', 'zt', 'jt', 'rs', 'jh', 'gn', 'zq', 'wx', 'kd', 'lb', 'nh', 'rd', 'fs', 'nx', 'rh', 'tb', 'jx', 'yc', 'wh', 'gl', 'nl', 'qb', 'dsh', 'wy', 'jc', 'wg', 'fx', 'mb', 'yt', 'fd', 'bb', 'rg', 'wj', 'xh', 'ym', 'bl', 'yn', 'wt', 'gx', 'zf', 'zr', 'bx', 'kl', 'zds', 'tl', 'ml', 'bg', 'bh', 'wn', 'es', 'mm', 'zjs', 'nj', 'nk', 'gj', 'zjd', 'ny', 'hy', 'tx', 'zyd', 'hx', 'wq', 'cx', 'zk', 'jy', 'ww', 'za', 'jr', 'gg', 'dl', 'nr', 'tt', 'fl', 'mn', 'cc', 'yk', 'th', 'br', 'bf', 'zzd', 'qn', 'jm', 'jn', 'tg', 'bn', 'yf', 'zp', 'lg', 'wc', 'dss', 'zgs', 'yr', 'dx', 'qc', 'wk', 'qg', 'ze', 'xg', 'fc', 'zys', 'zss', 'ln', 'mg', 'dh', 'qk', 'bj', 'wzd', 'as', 'qx', 'an', 'bc', 'jss', 'lx', 'hn', 'kn', 'tj', 'ps', 'zzy', 'zzs', 'dys', 'hg', 'hj', 'cj', 'mt', 'bq', 'mx', 'cg', 'wmd', 'kk', 'bt', 'pd', 'mh', 'lw', 'bss', 'rw', 'lj', 'ad', 'zzg', 'lh', 'qt', 'yss', 'rn', 'ygs', 'qj', 'gc', 'yds', 'zms', 'gw', 'zdd', 'mw', 'ky', 'lt', 'zmd', 'rj', 'ht', 'qq', 'dyg', 'ly', 'bk', 'wf', 'lm', 'by', 'ne', 'nw', 'wms', 'aw', 'xy', 'tc', 'hc', 'ty', 'qw', 'bw', 'xj', 'xc', 'zbs', 'zzj', 'wsm', 'ddd', 'xf', 'xw', 'zzm', 'qh', 'ysm', 'qy', 'jf', 'ed', 'nzm', 'nt', 'el', 'bzd', 'jk', 'qm', 'nc', 'zls', 'rl', 'zwm', 'yyd', 'xm', 'gt', 'hm', 'cf', 'xb', 'hw', 'jw', 'pl', 'nmd', 'zgd', 'bm', 'rc', 'py', 'sss', 'xq', 'jq', 'nzd', 'nq', 'zsd', 'rb', 'kj', 'dsd', 'rt', 'nms', 'gm', 'gh', 'wjd', 'fm', 'ydy', 'zhd', 'gb', 'gr', 'zhs', 'zdl', 'yjs', 'xn', 'ch', 'xt', 'ysd', 'cq', 'rm', 'dyd', 'lk', 'hk', 'fg', 'zxs', 'zxx', 'zws', 'fq', 'ssd', 'cm', 'cb', 'jsd', 'hq', 'dsq', 'ddx', 'zxd', 'yyg', 'ff', 'zns', 'gq', 'yzd', 'dsj', 'yp', 'tn', 'zzl', 'tk', 'mj', 'zcs', 'mys', 'ya', 'cw', 'ygr', 'yzy', 'zwd', 'zml', 'fh', 'zbd', 'fb', 'fj', 'nds', 'xxd', 'ygd', 'rx', 'tmd', 'zyg', 'mq', 'eq', 'zld', 'ct', 'zhl', 'zze', 'zyq', 'dyx', 'la', 'cn', 'tw', 'ssm', 'bsd', 'njs', 'lc', 'lsd', 'ydd', 'jds', 'ha', 'na', 'zyb', 'wds', 'kx', 'hsd', 'znd', 'wzj', 'tms', 'ba', 'yzs', 'mc', 'hr', 'dse', 'xsd', 'qa', 'nsd', 'ck', 'dsl', 'ybs', 'tr', 'tq', 'nzs', 'fk', 'yys', 'wzs', 'jdd', 'znm', 'pp', 'xxs', 'gsd', 'hzs', 'wsd', 'wjs', 'mf', 'nys', 'jdl', 'xr', 'gf', 'zmb', 'hys', 'hzd', 'ybd', 'nf', 'nzj', 'aq', 'hf', 'kb', 'he', 'cy', 'jzd', 'ngs', 'xzd', 'zts', 'hzy', 'qf', 'ja', 'ztd', 'wss', 'xa', 'nzg', 'wr', 'np', 'hp', 'ddl', 'jzy', 'jsl', 'gy', 'mk', 'ry', 'myd']\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  BASIC UTILITIES\n",
    "# =========================\n",
    "\n",
    "def is_chinese_char(ch: str) -> bool:\n",
    "    \"\"\"Rudimentary check if ch is a CJK Unified Ideograph.\"\"\"\n",
    "    return \"\\u4e00\" <= ch <= \"\\u9fff\"\n",
    "\n",
    "\n",
    "def char_to_initial(ch: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Convert a single Chinese character to the first letter of its pinyin.\n",
    "    Example: '我' -> 'w', '要' -> 'y'\n",
    "    Returns None if pinyin can't be obtained.\n",
    "    \"\"\"\n",
    "    if not is_chinese_char(ch):\n",
    "        return None\n",
    "\n",
    "    py_list = pinyin(ch, style=Style.NORMAL, strict=False)\n",
    "    if not py_list or not py_list[0]:\n",
    "        return None\n",
    "\n",
    "    syllable = py_list[0][0]  # e.g. 'wo', 'yao', 'shui'\n",
    "    return syllable[0].lower()  # 'w', 'y', 's', ...\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  NON-OVERLAPPING COUNTS\n",
    "# =========================\n",
    "\n",
    "def segment_initials_longest(initials: str, patterns: Iterable[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Greedy longest-match segmentation over an initials string.\n",
    "\n",
    "    - initials: e.g. 'ygydz...'\n",
    "    - patterns: iterable of valid patterns (e.g. TOKEN_PATTERNS)\n",
    "    Returns: list of segments like ['yg', 'y', 'd', ...]\n",
    "    \"\"\"\n",
    "    initials = initials.strip().lower()\n",
    "    n = len(initials)\n",
    "    i = 0\n",
    "    segments: List[str] = []\n",
    "\n",
    "    pat_set = set(pat for pat in patterns if pat)\n",
    "    if not pat_set:\n",
    "        return list(initials)\n",
    "\n",
    "    max_len = max(len(p) for p in pat_set)\n",
    "\n",
    "    while i < n:\n",
    "        matched = None\n",
    "        # try longest candidate first\n",
    "        for L in range(max_len, 0, -1):\n",
    "            if i + L > n:\n",
    "                continue\n",
    "            cand = initials[i:i + L]\n",
    "            if cand in pat_set:\n",
    "                matched = cand\n",
    "                break\n",
    "\n",
    "        if matched is None:\n",
    "            # fallback: unknown single letter\n",
    "            matched = initials[i]\n",
    "\n",
    "        segments.append(matched)\n",
    "        i += len(matched)\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "def build_non_overlapping_counts(\n",
    "    texts: Iterable[str],\n",
    "    patterns: Iterable[str],\n",
    ") -> Tuple[Dict[str, Counter], Dict[str, Counter]]:\n",
    "    \"\"\"\n",
    "    Build two frequency tables using non-overlapping segmentation:\n",
    "\n",
    "      - single_initial_counts: 'y'  -> Counter({'有': ..., '要': ..., '也': ...})\n",
    "      - pattern_word_counts:   'yg' -> Counter({'一个': ..., '一共': ...})\n",
    "\n",
    "    Pipeline per line:\n",
    "      1. Filter out non-Chinese characters.\n",
    "      2. For remaining characters, compute initials (1 per char).\n",
    "      3. Segment initials string using greedy longest-match over TOKEN_PATTERNS.\n",
    "      4. For each segment:\n",
    "         - length == 1: update single_initial_counts[initial][char]\n",
    "         - length > 1:  update pattern_word_counts[pattern][multi_char_word]\n",
    "\n",
    "    This ensures:\n",
    "      - multi-letter patterns \"claim\" their positions,\n",
    "      - single initials are trained only on positions not part of a multi-pattern.\n",
    "    \"\"\"\n",
    "    single_initial_counts: Dict[str, Counter] = defaultdict(Counter)\n",
    "    pattern_word_counts: Dict[str, Counter] = defaultdict(Counter)\n",
    "\n",
    "    for text in texts:\n",
    "        # Keep only Chinese chars, with their initials\n",
    "        chars: List[str] = []\n",
    "        initials: List[str] = []\n",
    "\n",
    "        for ch in text:\n",
    "            init = char_to_initial(ch)\n",
    "            if init is None:\n",
    "                # ignore non-Chinese or un-mappable chars for stats\n",
    "                continue\n",
    "            chars.append(ch)\n",
    "            initials.append(init)\n",
    "\n",
    "        if not chars:\n",
    "            continue\n",
    "\n",
    "        hanzi_seq = \"\".join(chars)\n",
    "        initials_seq = \"\".join(initials)\n",
    "\n",
    "        # segment initials into patterns\n",
    "        segs = segment_initials_longest(initials_seq, patterns)\n",
    "        pos = 0\n",
    "\n",
    "        for seg in segs:\n",
    "            L = len(seg)\n",
    "            if pos + L > len(hanzi_seq):\n",
    "                break  # safety\n",
    "\n",
    "            hanzi_chunk = hanzi_seq[pos:pos + L]\n",
    "            pos += L\n",
    "\n",
    "            if L == 1:\n",
    "                ini = seg\n",
    "                char = hanzi_chunk  # single character\n",
    "                single_initial_counts[ini][char] += 1\n",
    "            else:\n",
    "                pattern_word_counts[seg][hanzi_chunk] += 1\n",
    "\n",
    "    return single_initial_counts, pattern_word_counts\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  SIMPLE PER-LETTER DECODER\n",
    "# =========================\n",
    "\n",
    "def build_initial_to_best_char(single_initial_counts: Dict[str, Counter]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    For each initial letter, pick the single most frequent character.\n",
    "    Example: {'w': Counter({'我': 1234, '问': 200, ...})}\n",
    "    -> {'w': '我'}\n",
    "    \"\"\"\n",
    "    best: Dict[str, str] = {}\n",
    "    for initial, counter in single_initial_counts.items():\n",
    "        if not counter:\n",
    "            continue\n",
    "        most_common_char, _ = counter.most_common(1)[0]\n",
    "        best[initial] = most_common_char\n",
    "    return best\n",
    "\n",
    "\n",
    "def decode_initial_sequence(\n",
    "    initials: str,\n",
    "    initial_to_best_char: Dict[str, str],\n",
    "    unknown_char: str = \"□\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Convert an initial-letter sequence (e.g. 'wysj')\n",
    "    to a Chinese string using a simple per-initial mapping.\n",
    "\n",
    "    unknown_char: used when an initial is not in the learned mapping.\n",
    "    \"\"\"\n",
    "    result_chars = []\n",
    "    for letter in initials.lower().strip():\n",
    "        result_chars.append(initial_to_best_char.get(letter, unknown_char))\n",
    "    return \"\".join(result_chars)\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  MULTI-PATTERN DECODER (DP)\n",
    "# =========================\n",
    "\n",
    "def build_pattern_to_best_word(\n",
    "    single_initial_counts: Dict[str, Counter],\n",
    "    pattern_word_counts: Dict[str, Counter],\n",
    "    min_count_single: int = 1,\n",
    "    min_count_multi: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine:\n",
    "      - single_initial_counts: 'y'  -> Counter of single chars\n",
    "      - pattern_word_counts:   'yg' -> Counter of multi-char words\n",
    "\n",
    "    into:\n",
    "      - pattern_to_word:  'y'  -> '有', 'yg' -> '一个', ...\n",
    "      - pattern_to_score: log(count(best_candidate))\n",
    "      - max_pattern_len:  maximum length of any pattern\n",
    "    \"\"\"\n",
    "    pattern_to_word: Dict[str, str] = {}\n",
    "    pattern_to_score: Dict[str, float] = {}\n",
    "    max_len = 0\n",
    "\n",
    "    # single-letter patterns\n",
    "    for pat, counter in single_initial_counts.items():\n",
    "        if not counter:\n",
    "            continue\n",
    "        best_word, cnt = counter.most_common(1)[0]\n",
    "        if cnt < min_count_single:\n",
    "            continue\n",
    "        pattern_to_word[pat] = best_word\n",
    "        pattern_to_score[pat] = math.log(cnt)\n",
    "        max_len = max(max_len, len(pat))\n",
    "\n",
    "    # multi-letter patterns\n",
    "    for pat, counter in pattern_word_counts.items():\n",
    "        if not counter:\n",
    "            continue\n",
    "        best_word, cnt = counter.most_common(1)[0]\n",
    "        if cnt < min_count_multi:\n",
    "            continue\n",
    "        pattern_to_word[pat] = best_word\n",
    "        pattern_to_score[pat] = math.log(cnt)\n",
    "        max_len = max(max_len, len(pat))\n",
    "\n",
    "    return pattern_to_word, pattern_to_score, max_len\n",
    "\n",
    "\n",
    "def decode_initials_dp(\n",
    "    initials: str,\n",
    "    pattern_to_word: Dict[str, str],\n",
    "    pattern_to_score: Dict[str, float],\n",
    "    max_pattern_len: int,\n",
    "    unknown_char: str = \"□\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Dynamic programming decoder for initials → Hanzi.\n",
    "\n",
    "    It chooses the segmentation and sequence of patterns that maximize\n",
    "    the sum of log-count scores.\n",
    "    \"\"\"\n",
    "    initials = initials.strip().lower()\n",
    "    n = len(initials)\n",
    "\n",
    "    best = [-math.inf] * (n + 1)\n",
    "    back: List[str | None] = [None] * (n + 1)\n",
    "    best[0] = 0.0\n",
    "\n",
    "    for i in range(n):\n",
    "        if best[i] == -math.inf:\n",
    "            continue\n",
    "        for L in range(1, max_pattern_len + 1):\n",
    "            j = i + L\n",
    "            if j > n:\n",
    "                break\n",
    "\n",
    "            pat = initials[i:j]\n",
    "            if pat not in pattern_to_word:\n",
    "                continue\n",
    "\n",
    "            score = best[i] + pattern_to_score[pat]\n",
    "            if score > best[j]:\n",
    "                best[j] = score\n",
    "                back[j] = pat\n",
    "\n",
    "    if best[n] == -math.inf:\n",
    "        return unknown_char * n\n",
    "\n",
    "    # backtracking\n",
    "    out: List[str] = []\n",
    "    pos = n\n",
    "    while pos > 0:\n",
    "        pat = back[pos]\n",
    "        if pat is None:   # safeguard\n",
    "            out.append(unknown_char)\n",
    "            pos -= 1\n",
    "            continue\n",
    "        out.append(pattern_to_word[pat])\n",
    "        pos -= len(pat)\n",
    "\n",
    "    out.reverse()\n",
    "    return \"\".join(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b3311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "#  TRAIN STATS ON YOUR CSV\n",
    "# =========================\n",
    "\n",
    "training_texts = zho_text_cleaned\n",
    "\n",
    "# 1) Build clean, non-overlapping statistics\n",
    "single_initial_counts, pattern_word_counts = build_non_overlapping_counts(\n",
    "    training_texts,\n",
    "    patterns=TOKEN_PATTERNS,\n",
    ")\n",
    "\n",
    "# 2) Simple one-letter mapping\n",
    "initial_to_best_char = build_initial_to_best_char(single_initial_counts)\n",
    "\n",
    "# 3) Multi-pattern DP decoder tables\n",
    "pattern_to_word, pattern_to_score, max_pattern_len = build_pattern_to_best_word(\n",
    "    single_initial_counts,\n",
    "    pattern_word_counts,\n",
    "    min_count_single=1,\n",
    "    min_count_multi=2,\n",
    ")\n",
    "\n",
    "# 20 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0702ff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL HANZI (first 80): 来自南方都市报播报武汉大学老牌坊被撞损一事持续引发关注 六月八十日 南都记者从武汉市公安局洪山分局一位工作人员处获悉 撞损牌坊的肇事司机因涉嫌过失损毁文物罪被依\n",
      "INITIALS         (first 80): lznfdsbbbwhdxlpfbzsyscxyfgzlybsrndjzcwhsgajhsfjywgzrychxzspfdzssjysxgsshwwzbyfxs\n",
      "BASELINE (per-letter)      : 了长你方的是不不不我孩的现了平方不长是一是成现一方个长了一不是人你的就长成我孩是个啊就孩是方就一我个长人一成孩现长是平方的长是是就一是现个是是孩我我长不一方现是\n",
      "DP MULTI-PATTERN DECODER   : 了长你方的是不不不我孩的现了平方不长是一是成现一方个长了一不是人你的就长成我孩是个啊就孩是方就一我个长人一成孩现长是平方的长是是就一是现个是是孩我我长不一方现是\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Tiny smoke test: use the first line of your corpus\n",
    "    if training_texts:\n",
    "        sample_hanzi = training_texts[0]\n",
    "\n",
    "        # build initials for the Chinese chars only (for decoding test)\n",
    "        initials_seq = \"\".join(\n",
    "            init\n",
    "            for init in (char_to_initial(ch) for ch in sample_hanzi)\n",
    "            if init is not None\n",
    "        )\n",
    "\n",
    "        print(\"ORIGINAL HANZI (first 80):\", sample_hanzi[:80])\n",
    "        print(\"INITIALS         (first 80):\", initials_seq[:80])\n",
    "\n",
    "        baseline = decode_initial_sequence(initials_seq, initial_to_best_char)\n",
    "\n",
    "        decoded_dp = decode_initials_dp(\n",
    "            initials=initials_seq,\n",
    "            pattern_to_word=pattern_to_word,\n",
    "            pattern_to_score=pattern_to_score,\n",
    "            max_pattern_len=max_pattern_len,\n",
    "        )\n",
    "\n",
    "        print(\"BASELINE (per-letter)      :\", baseline[:80])\n",
    "        print(\"DP MULTI-PATTERN DECODER   :\", decoded_dp[:80])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
