{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40e5a30",
   "metadata": {},
   "source": [
    "# train.ipynb\n",
    "This notebook contains the training code for a small language model using the TRL (Transformer Reinforcement Learning) library.\n",
    "\n",
    "We'll be training a decoder-only (GPT-2 style) model with a custom configuration and using the DebertaV2 tokenizer provided in the data folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac64635c",
   "metadata": {},
   "source": [
    "## Installation\n",
    "First, we need to install the required dependencies for training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cbe38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, DebertaV2Tokenizer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"Extract predicted token IDs from model logits for evaluation\"\"\"\n",
    "    pred_ids = torch.argmax(logits, dim=-1)  # Get the token with highest probability\n",
    "    return pred_ids\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate accuracy by comparing predictions with true labels\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.flatten()  # Flatten to 1D array\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    # Only consider non-padding tokens (labels != -100 are actual tokens)\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    predictions = predictions[mask]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = labels == predictions\n",
    "    accuracy = correct.sum() / float(len(correct))\n",
    "    return {\"acc\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e64e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the custom tokenizer trained on BabyLM dataset\n",
    "model_name = \"openai-community/gpt2\"\n",
    "tokenizer = DebertaV2Tokenizer('data/tokenizer.model')\n",
    "\n",
    "# Create custom configuration based on GPT-2 but with smaller dimensions\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.hidden_size = 384 # Same dimensionality as the best performing BabyLM model.\n",
    "config.intermediate_size = 1280  # Feed-forward intermediate size\n",
    "config.vocab_size = tokenizer.vocab_size  # Match tokenizer vocabulary size\n",
    "\n",
    "# Initialize model with custom configuration\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# Print model size for reference, should be around 31M parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of model parameters: {num_params}\")\n",
    "\n",
    "# Load training and validation datasets\n",
    "dataset = load_dataset('text', data_files = {'train': 'data/train.txt', 'validation': 'data/dev.txt'})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a109ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "set_seed(0)\n",
    "\n",
    "# Initialize SFTTrainer with comprehensive configuration\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,  # Tokenizer for text processing\n",
    "    train_dataset = dataset['train'],\n",
    "    eval_dataset = dataset['validation'],\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args = SFTConfig(\n",
    "        # Data processing\n",
    "        remove_unused_columns = True,\n",
    "        label_names = [\"labels\"],\n",
    "        dataset_num_proc = 12,  # Number of processes for dataset preprocessing\n",
    "        packing = True,  # Pack multiple sequences into single training example\n",
    "        eval_packing = True,\n",
    "        max_length = 64,  # Maximum sequence length\n",
    "        dataset_text_field = \"text\",\n",
    "        \n",
    "        # Training strategy\n",
    "        eval_strategy = \"steps\",\n",
    "        per_device_train_batch_size = 64, # how many sequences to process at once\n",
    "        gradient_accumulation_steps = 1, # how many batches to process before updating the model. Effectively increases the batch size without increasing memory usage.\n",
    "        warmup_ratio = 0.05,  # 5% of training steps for learning rate increase from 0to 2e-4\n",
    "        num_train_epochs = 10,\n",
    "        learning_rate = 2e-4,\n",
    "        \n",
    "        # Optimization and precision\n",
    "        fp16 = True,  # Use half precision for memory efficiency\n",
    "        bf16 = False,\n",
    "        optim = \"adamw_8bit\",  # 8-bit AdamW optimizer\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",  # Cosine learning rate schedule\n",
    "        max_grad_norm=1,  # Gradient clipping\n",
    "        \n",
    "        # Logging and evaluation\n",
    "        logging_steps = 10,\n",
    "        eval_steps = 100,\n",
    "        save_steps = 100,\n",
    "        eval_accumulation_steps=1, # how many evaluation batches to accumulate on GPU. \n",
    "        include_for_metrics=[],\n",
    "        \n",
    "        # Reproducibility and output\n",
    "        seed = 0,\n",
    "        # output_dir = \"\",  # Uncomment to specify output directory\n",
    "        report_to = \"none\",  # Disable wandb/tensorboard logging\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
